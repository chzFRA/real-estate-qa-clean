# build_index_stream.py
"""
é€æ–‡ä»¶â†’é€ batch å†™ FAISSï¼Œé¿å…ä¸€æ¬¡åƒå…‰å†…å­˜
ä¾èµ–: pip install sentence-transformers faiss-cpu pandas tqdm
"""

import os, glob, json, gc
import pandas as pd
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import faiss

TXT_DIR   = "texts_clean"
INDEX_DIR = "vector_index"
os.makedirs(INDEX_DIR, exist_ok=True)

CHUNK_SIZE = 800      # å­—ç¬¦
OVERLAP    = 100
BATCH_SIZE = 64       # å‘é‡åŒ– batch

def lazy_chunk(text):
    """ç”Ÿæˆå™¨ï¼šæŒ‰ CHUNK_SIZE æ»‘çª—åˆ‡ç‰‡ï¼Œyield chunk å­—ç¬¦ä¸²"""
    start = 0
    ln = len(text)
    while start < ln:
        end = min(start + CHUNK_SIZE, ln)
        yield text[start:end]
        start = end - OVERLAP

# ---------- åˆå§‹åŒ– FAISS ----------
model = SentenceTransformer("BAAI/bge-base-zh")
d = model.get_sentence_embedding_dimension()
index = faiss.IndexFlatL2(d)          # ä¹Ÿå¯ä»¥æ¢æˆ IndexHNSWFlat ç­‰

# åœ¨è¿™åŒæ—¶æŠŠå…ƒæ•°æ®å†™ parquetï¼ˆè¿½åŠ å¼ï¼‰
meta_path = os.path.join(INDEX_DIR, "meta.parquet")
meta_df_list = []

# ---------- é€æ–‡ä»¶æµå¼å¤„ç† ----------
for txt_path in tqdm(glob.glob(os.path.join(TXT_DIR, "*.txt"))):
    with open(txt_path, encoding="utf-8") as f:
        text = f.read()
    basename = os.path.basename(txt_path)

    # æŒ‰ batch åš embedding
    buffer = []
    meta_buffer = []
    for i, chunk in enumerate(lazy_chunk(text)):
        buffer.append(chunk)
        meta_buffer.append({"doc": basename, "chunk_id": i})
        # æ»¡ä¸€ä¸ª batch æˆ–æœ€åä¸€ä¸ª chunk æ—¶ flush
        if len(buffer) == BATCH_SIZE or (i == 0 and len(text) < CHUNK_SIZE):
            print(f"ğŸ§  æ­£åœ¨ç¼–ç  {basename} ä¸­çš„ç¬¬ {i+1} å—ï¼Œå…± {len(buffer)} æ¡...")
            embs = model.encode(buffer, batch_size=len(buffer))
            index.add(embs)
            meta_df_list.extend(meta_buffer)
            buffer.clear()
            meta_buffer.clear()
            gc.collect()

    # flush ä½™ä¸‹ä¸è¶³ batch çš„
    if buffer:
        embs = model.encode(buffer, batch_size=len(buffer))
        index.add(embs)
        meta_df_list.extend(meta_buffer)
        buffer.clear()
        meta_buffer.clear()
        gc.collect()

# ---------- ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ® ----------
faiss.write_index(index, os.path.join(INDEX_DIR, "legal_index.faiss"))
pd.DataFrame(meta_df_list).to_parquet(meta_path, index=False)
print(f"ğŸ‰ ç´¢å¼•å®Œæˆï¼å…± {index.ntotal} æ¡å‘é‡ï¼Œå…ƒæ•°æ®å·²å†™ {meta_path}")

import os, json
from pathlib import Path
from tqdm import tqdm
from llm_utils import call_model

INDEX_DIR = Path("vector_index")
MCQ_DIR = Path("mcq_clean")
MODEL = "gpt4"  # é»˜è®¤ä½¿ç”¨çš„æ¨¡åž‹ï¼Œå¯ä»¥åˆ‡æ¢ gpt3.5 / gpt4 / claude / deepseek

def build_prompt(q_obj):
    q = q_obj["question"]
    opts = q_obj["options"]
    return f"""ä½ æ˜¯ä¸€åæˆ¿åœ°äº§æ³•å¾‹è€ƒè¯•ä¸“å®¶ï¼Œæ ¹æ®ä»¥ä¸‹é—®é¢˜é€‰æ‹©æ­£ç¡®ç­”æ¡ˆï¼š

é—®é¢˜ï¼š{q}

A. {opts['A']}
B. {opts['B']}
C. {opts['C']}
D. {opts['D']}

è¯·ç›´æŽ¥è¿”å›žå­—æ¯ A/B/C/Dï¼Œè¡¨ç¤ºæ­£ç¡®é€‰é¡¹ï¼Œæ— éœ€è§£é‡Šã€‚"""

def evaluate_model(mcq_file):
    with open(mcq_file, "r", encoding="utf-8") as f:
        mcqs = json.load(f)

    correct = 0
    total = len(mcqs)

    for q_obj in tqdm(mcqs, desc=f"ðŸ“ æ­£åœ¨è¯„ä¼° {mcq_file.name}"):
        prompt = build_prompt(q_obj)
        pred = call_model(prompt, model=MODEL).strip().upper()[0]
        if pred == q_obj["correct_answer"]:
            correct += 1

    acc = correct / total if total > 0 else 0
    print(f"\nðŸ“Š æ–‡ä»¶ï¼š{mcq_file.name} | å‡†ç¡®çŽ‡ï¼š{correct}/{total} = {acc:.2%}")

if __name__ == "__main__":
    for mcq_file in MCQ_DIR.glob("*.json"):
        evaluate_model(mcq_file)

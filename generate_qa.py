import os
import json
from llm_utils import call_model

INPUT_DIR = "texts_clean"
OUTPUT_DIR = "results"
ERROR_LOG = "error.log"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# åˆ†å—å‡½æ•°ï¼šå°½é‡åœ¨å¥å­è¾¹ç•Œåˆ‡æ–­
def chunk_text(text, max_chunk_size=2000):
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + max_chunk_size, len(text))
        while end < len(text) and text[end] not in ".ã€‚ï¼ï¼Ÿ\n":
            end += 1
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        start = end
    return chunks

# é€‰æ‹©é¢˜ prompt
def generate_mcq(chunk, model="deepseek"):
    prompt = f"""ä½ æ˜¯ä¸€åæ³•å¾‹è€ƒè¯•é¢˜åº“è®¾è®¡ä¸“å®¶ã€‚è¯·é˜…è¯»ä»¥ä¸‹æœ‰å…³äºæˆ¿åœ°äº§çš„æ³•å¾‹æ¡æ–‡,è®¾è®¡1é“é€‰æ‹©é¢˜,è¦æ±‚å¦‚ä¸‹ï¼š

1. é—®é¢˜è´´è¿‘æ™®é€šæˆ¿åœ°äº§ç”¨æˆ·å¯èƒ½ä¼šé—®çš„é—®é¢˜ï¼›
2. ç»™å‡º4ä¸ªé€‰é¡¹(A/B/C/D),å…¶ä¸­ä¸€ä¸ªæ˜¯æ­£ç¡®ç­”æ¡ˆï¼Œå…¶ä½™ä¸ºå¹²æ‰°é¡¹ï¼›
3. æ‰€æœ‰é€‰é¡¹å¿…é¡»åŸºäºæ¡æ–‡å†…å®¹ï¼›
4. æŒ‡å‡ºå“ªä¸€ä¸ªé€‰é¡¹æ˜¯æ­£ç¡®ç­”æ¡ˆï¼›
5. ä½¿ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€,ä¸è¦æ­»æ¿å¹¶ä¸”ä½“ç°å‡ºdiversityå’Œä¸ªæ€§åŒ–ã€‚

ä»¥ä¸‹æ˜¯æ³•å¾‹æ¡æ–‡ï¼š
{chunk}

è¯·ç”¨å¦‚ä¸‹æ ¼å¼è¾“å‡ºï¼š
Q: ...
A. ...
B. ...
C. ...
D. ...
æ­£ç¡®ç­”æ¡ˆ: X
"""
    return call_model(prompt, model=model)

# ä¸»æµç¨‹
if __name__ == "__main__":
    model = "deepseek"

    with open(ERROR_LOG, "w", encoding="utf-8") as log:
        for filename in os.listdir(INPUT_DIR):
            if not filename.endswith(".txt"):
                continue

            input_path = os.path.join(INPUT_DIR, filename)
            base_name = filename.replace(".txt", f"_{model}_mcq")
            output_txt = os.path.join(OUTPUT_DIR, base_name + ".txt")
            output_json = os.path.join(OUTPUT_DIR, base_name + ".json")

            if os.path.exists(output_txt) and os.path.exists(output_json):
                print(f"âš ï¸ å·²å­˜åœ¨ï¼Œè·³è¿‡ï¼š{base_name}")
                continue

            with open(input_path, "r", encoding="utf-8") as f:
                full_text = f.read()

            chunks = chunk_text(full_text)
            results = []

            for i, chunk in enumerate(chunks):
                print(f"\nğŸ“¦ æ­£åœ¨å¤„ç† {filename} çš„ç¬¬ {i+1} å—æ–‡æœ¬...")
                try:
                    mcq = generate_mcq(chunk, model=model)
                    results.append({
                        "chunk_id": i,
                        "mcq_text": mcq,
                        "source": chunk.strip()[:100] + "..."
                    })
                except Exception as e:
                    err_msg = f"âŒ å‡ºé”™æ–‡ä»¶ {filename}ï¼Œå— {i}ï¼ŒåŸå› ï¼š{e}\n"
                    print(err_msg)
                    log.write(err_msg)

            # ä¿å­˜çº¯æ–‡æœ¬
            with open(output_txt, "w", encoding="utf-8") as f:
                for r in results:
                    f.write(f"------ Chunk {r['chunk_id']} ------\n")
                    f.write(r['mcq_text'] + "\n\n")
            print(f"âœ… å·²ä¿å­˜çº¯æ–‡æœ¬é€‰æ‹©é¢˜ï¼š{output_txt}")

            # ä¿å­˜ç»“æ„åŒ– JSON
            with open(output_json, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"âœ… å·²ä¿å­˜ç»“æ„åŒ– JSONï¼š{output_json}")

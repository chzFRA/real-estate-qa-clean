"""
build_index_incremental.py
--------------------------
æ‰«æ texts_clean/ ä¸‹çš„ *_cleaned.txtï¼Œ
ä¸ºæ¯ä¸ªæ–‡ä»¶æ„å»º Faiss å‘é‡ç´¢å¼•ï¼Œå¹¶æŠŠ
   - chunk_text
   - doc (æ–‡ä»¶ id)
   - chunk_id
ä¿å­˜åˆ°åŒå *_meta.parquetã€‚
é‡å¤æ‰§è¡Œä¼šè‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨çš„ .faissï¼ˆé™¤éè®¾ç½® FORCE_REBUILDï¼‰ã€‚
"""

from pathlib import Path
import faiss, json, numpy as np, pandas as pd
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# ==== é…ç½® ====
TXT_DIR       = Path("texts_clean")
INDEX_DIR     = Path("vector_index")
EMBED_MODEL   = "paraphrase-multilingual-MiniLM-L12-v2"
CHUNK_LEN     = 200          # ä¸ä¹‹å‰ä¿æŒä¸€è‡´
FORCE_REBUILD = True         # True=æ— æ¡ä»¶é‡å»ºï¼ŒFalse=å·²å­˜åœ¨åˆ™è·³è¿‡

INDEX_DIR.mkdir(exist_ok=True)
model = SentenceTransformer(EMBED_MODEL)

def build_index_for_file(txt_path: Path):
    fname = txt_path.stem                           # e.g. 096be8ed81xxxx_cleaned
    faiss_path = INDEX_DIR / f"{fname}.faiss"
    meta_path  = INDEX_DIR / f"{fname}_meta.parquet"

    if not FORCE_REBUILD and faiss_path.exists() and meta_path.exists():
        return

    # --- è¯»æ–‡æœ¬å¹¶åˆ‡å— ---
    chunks = []
    with txt_path.open(encoding="utf-8") as f:
        for i, line in enumerate(f):
            line = line.strip()
            if not line:
                continue
            # å¾ˆç®€å•åœ°æ¯ CHUNK_LEN å­—ç¬¦ä¸€å—
            for start in range(0, len(line), CHUNK_LEN):
                chunk = line[start : start + CHUNK_LEN]
                chunks.append({"chunk_text": chunk,
                               "doc": fname,
                               "chunk_id": len(chunks)})

    if not chunks:
        print(f"âš ï¸  {fname} æ— å†…å®¹ï¼Œè·³è¿‡")
        return

    # --- å‘é‡åŒ– ---
    texts   = [c["chunk_text"] for c in chunks]
    vectors = model.encode(texts, batch_size=64, show_progress_bar=True).astype("float32")

    # --- å†™ Faiss ---
    dim = vectors.shape[1]
    index = faiss.IndexFlatIP(dim)
    faiss.normalize_L2(vectors)
    index.add(vectors)
    faiss.write_index(index, str(faiss_path))

    # --- å†™ meta ---
    pd.DataFrame(chunks).to_parquet(meta_path, engine="pyarrow")

    print(f"ğŸ‰ å®Œæˆ {fname}ï¼š{len(chunks)} å‘é‡ -> {faiss_path.name}")

def main():
    txt_files = sorted(TXT_DIR.glob("*_cleaned.txt"))
    print(f"ğŸ“š å…± {len(txt_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶\n")
    for f in tqdm(txt_files, desc="æ–‡ä»¶è¿›åº¦"):
        build_index_for_file(f)
    print("ğŸ å…¨éƒ¨æ³•è§„å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()

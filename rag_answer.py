import os, glob, json, argparse
from pathlib import Path
import pandas as pd
from tqdm import tqdm
import faiss
from sentence_transformers import SentenceTransformer
from llm_utils import call_model

# === è·¯å¾„é…ç½® ===
INDEX_DIR = Path("vector_index")
EMBED_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"
TOP_K = 5  # æ£€ç´¢æ¡æ–‡ç‰‡æ®µæ•°é‡

# === å‘é‡æ£€ç´¢ & åŠ è½½ ===
def load_indexes():
    indexes, metas = [], []
    for faiss_fp in INDEX_DIR.glob("*.faiss"):
        meta_fp = faiss_fp.with_name(f"{faiss_fp.stem}_meta.parquet")
        if not meta_fp.exists():
            continue
        index = faiss.read_index(str(faiss_fp))
        meta = pd.read_parquet(meta_fp, engine="pyarrow")
        indexes.append(index)
        metas.append(meta)
    return indexes, metas

INDEXES, METAS = load_indexes()
MODEL = SentenceTransformer(EMBED_MODEL)

# === æ£€ç´¢ç›¸å…³æ¡æ–‡ ===
def retrieve_context(question: str, indexes, metas, model, top_k=5):
    q_vec = model.encode([question]).astype("float32")
    context_chunks = []
    for idx, index in enumerate(indexes):
        D, I = index.search(q_vec, top_k)
        meta = metas[idx]
        for _, ix in zip(D[0], I[0]):
            if ix == -1: continue
            row = meta.iloc[ix]
            chunk = row.get("chunk_text", row.get("doc", ""))  # å°è¯•å– chunk_textï¼Œæ²¡æœ‰åˆ™å– doc åˆ—
            context_chunks.append(chunk.strip())
    return "\n\n".join(context_chunks)

# === æ„å»º Prompt ===
def build_prompt(question: str, context: str):
    return f"""ä½ æ˜¯ä¸€åæˆ¿åœ°äº§æ³•å¾‹ä¸“å®¶ï¼Œç»“åˆä»¥ä¸‹æ³•è§„å†…å®¹å›ç­”é—®é¢˜ï¼š

ã€æ³•è§„æ¡æ–‡ã€‘ï¼š
{context}

ã€é—®é¢˜ã€‘ï¼š
{question}

è¯·ç›´æ¥å›ç­”æ­£ç¡®é€‰é¡¹çš„å­—æ¯ï¼ˆA/B/C/Dï¼‰ï¼Œæ— éœ€è§£é‡Šã€‚"""

# === å•é¢˜æµ‹è¯• ===
def answer_one_question(q_text: str):
    ctx = retrieve_context(q_text, INDEXES, METAS, MODEL, top_k=TOP_K)
    prompt = build_prompt(q_text, ctx)
    model_ans = call_model(prompt, model="deepseek")
    print("\nğŸ§  LLM é€‰æ‹©çš„ç­”æ¡ˆï¼š", model_ans.strip())
    print("â€”â€”â€” å®Œæˆ â€”â€”\n")

# === æ‰¹é‡æµ‹è¯• ===
def run_batch(mcq_file):
    with open(mcq_file, "r", encoding="utf-8") as f:
        content = f.read()

    questions = content.split("------ Chunk")
    total = len(questions) - 1

    correct = 0
    for q in tqdm(questions[1:], desc="ğŸ“– æ‰¹é‡è¯„æµ‹ä¸­"):
        parts = q.strip().split("æ­£ç¡®ç­”æ¡ˆ:")
        if len(parts) < 2: continue
        q_text, correct_ans = parts[0], parts[1].strip()[0]
        ctx = retrieve_context(q_text, INDEXES, METAS, MODEL, top_k=TOP_K)
        prompt = build_prompt(q_text, ctx)
        pred_ans = call_model(prompt, model="deepseek").strip()[0]

        if pred_ans.upper() == correct_ans.upper():
            correct += 1

    print(f"\nâœ… æ‰¹é‡æµ‹è¯•å®Œæˆï¼Œå‡†ç¡®ç‡ï¼š{correct}/{total} = {correct/total:.2%}")

# === ä¸»ç¨‹åºå…¥å£ ===
if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--mcq_file", help="æ‰¹é‡è¯„æµ‹ï¼šæŒ‡å®šä¸€ä»½ _mcq.txt / .json æ–‡ä»¶")
    ap.add_argument("--top_k", type=int, default=5)
    args = ap.parse_args()

    if args.mcq_file:
        run_batch(args.mcq_file)
    else:
        print("âœ¨ è¿›å…¥äº¤äº’æµ‹è¯•ï¼šç›´æ¥ç²˜è´´ä¸€é“é¢˜ï¼ˆåŒ…å«å››ä¸ªé€‰é¡¹ï¼‰ï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼ŒCtrl+C é€€å‡º\n")
        try:
            while True:
                lines = []
                print("è¯·ç²˜è´´å®Œæ•´é¢˜ç›®ï¼ˆç©ºè¡Œç»“æŸï¼‰ï¼š")
                while True:
                    ln = input()
                    if not ln.strip(): break
                    lines.append(ln)
                q = "\n".join(lines).strip()
                if q:
                    answer_one_question(q)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ ~")
